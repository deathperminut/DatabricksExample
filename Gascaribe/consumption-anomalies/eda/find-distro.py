# Databricks notebook source
import pandas as pd
import scipy.stats as stats
from scipy.stats import kstest
import matplotlib.pyplot as plt

# COMMAND ----------

# Fetch the data from the table analiticagdc.desviacionconsumos.factconsumo into a pandas dataframe
data = spark.sql("SELECT CAST(ROUND(VolumenNormalizado) AS INT) AS Volumen FROM analiticagdc.desviacionconsumos.factconsumo WHERE FechaPeriodo = '2024-03-01' AND IdCategoria = 1 AND VolumenNormalizado > 0 AND VolumenNormalizado IS NOT NULL").toPandas()

# COMMAND ----------

import numpy as np
from scipy import stats

# Fit the sample data to various distributions
gamma_params = stats.gamma.fit(data)
norm_params = stats.lognorm.fit(data)
exp_params = stats.expon.fit(data)

# Calculate the AIC score for each distribution
gamma_aic = stats.gamma.nnlf(gamma_params, data)
norm_aic = stats.lognorm.nnlf(norm_params, data)
exp_aic = stats.expon.nnlf(exp_params, data)

# Determine the distribution with the lowest AIC score
if gamma_aic < norm_aic and gamma_aic < exp_aic:
    best_dist = 'gamma'
    best_params = gamma_params
elif norm_aic < gamma_aic and norm_aic < exp_aic:
    best_dist = 'lognorm'
    best_params = norm_params
else:
    best_dist = 'expon'
    best_params = exp_params

best_dist, best_params

# COMMAND ----------

# MAGIC %md
# MAGIC Se detecta que el consumo reside se ajusta a una distribuci칩n log-normal
# MAGIC Esta conclusi칩n se obtuvo utilizando una prueba de bondad de ajuste y posteriormente el criterio de Akaike (AIC) para escoger la distribuci칩n m치s cercana entre las tres candidatas (Gamma, LogNormal y Exponencial)

# COMMAND ----------

print("moda:", data.mode())
print("media:", data.mean())
print("desviacion:", data.std())

# COMMAND ----------

print("Rango limite superior valor aceptado:", (data.mode().iloc[0][0] + 2 * data.std().iloc[0]), ' - ', (data.mean().iloc[0] + 2 * data.std().iloc[0]))

# COMMAND ----------

# MAGIC %md
